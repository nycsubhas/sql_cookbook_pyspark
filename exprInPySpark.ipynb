{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe81740-b2df-4025-8a48-08d80937b0bb",
   "metadata": {},
   "source": [
    "# 1. PySpark expr() Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16040e-f503-4cae-81f8-4ed300a8db1f",
   "metadata": {},
   "source": [
    "## Syntax: expr(str) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fb96f77-8f0d-4914-a030-3dc037589182",
   "metadata": {},
   "source": [
    "expr() function takes sql expression as a string argument, executes the expression, and returns a PySpark Column type. Expressions provided with this function are not a compile-time safety like dataframe operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae52d4e-d627-4b47-98b8-b6a3f13316c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 12:11:02 WARN Utils: Your hostname, user-HP-Pavilion-x360-Convertible-14-dh0xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlo1)\n",
      "25/02/27 12:11:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/27 12:11:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/27 12:11:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63041546-b419-46bf-b537-8c892b02e1e6",
   "metadata": {},
   "source": [
    "## Concatenate Columns using || (similar to SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507ae2c1-a078-4996-992b-3c1b969f197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Bond\"),(\"Scott\",\"Varsa\")]\n",
    "df = spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.withColumn(\"Name\",expr(\"col1||','|| col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5c91b-2cdd-4c29-9c4f-94ae3ddd69bb",
   "metadata": {},
   "source": [
    "## Using SQL CASE WHEN with expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5117389-b8b3-4650-ba13-2fb9a9319ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJames\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMichael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      3\u001b[0m columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(data,columns)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expr \n\u001b[1;32m      8\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m,expr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase when gender=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m then \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen gender=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m then \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m end\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr \n",
    "data = [(\"James\",\"M\"),(\"Michael\", \"F\"),(\"Jen\",\"\")]\n",
    "columns=[\"names\",\"gender\"] \n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "from pyspark.sql.functions import expr \n",
    "df2 = df.withColumn(\"gender\",expr(\"case when gender='M' then 'Male'\" + \"when gender='F' then 'Female'\"+\"else 'unknown' end\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5bbe5-7a7c-4c9a-acb9-4b4d0b383c1b",
   "metadata": {},
   "source": [
    "## Using an Existing Column Value for Expression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a1ae1d1-3dfc-4f79-9432-3faa4cd5e51a",
   "metadata": {},
   "source": [
    "Most of the PySpark function takes constant literal values but somtimes we need to use a value \n",
    "from an existing column instead of a constant and this is not possible without expr() expression. The following example adds a numer of months from an existing column instead of a Python Constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e76cfa8-4608-44bd-abee-0067bd0ddf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df = spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "df.select(df.date, df.increment, expr(\"add_months(date,increment)\").alias(\"inc_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eeb527-a7d5-4de3-bed8-8159dff76ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
