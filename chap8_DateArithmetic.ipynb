{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afb51fc-1b2e-40a7-b12b-d3a8640524f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(7369, \"SMITH\", \"CLERK\", 7902, \"17-12-1980\", 800, None, 20),\n",
    "        (7499, \"ALLEN\", \"SALESMAN\",7698,\"20-02-1981\",1600,300,  30),\n",
    "        (7521, \"WARD\", \"SALESMAN\", 7698,\"22-02-1981\",1250,500,  30),\n",
    "        (7566, \"JONES\", \"MANAGER\", 7839, \"02-04-1981\",2975,None,20),\n",
    "        (7654, \"MARTIN\", \"SALESMAN\", 7698,\"28-09-1981\",1250,1400,30),\n",
    "        (7698, \"BLAKE\", \"MANAGER\", 7839, \"01-05-1981\", 2850, None,30),\n",
    "        (7782, \"CLARK\", \"MANAGER\", 7839, \"09-06-1981\", 2450, None, 10),\n",
    "        (7788, \"SCOTT\", \"ANALYST\", 7566, \"09-12-1982\", 3000, None, 20),\n",
    "        (7839, \"KING\", \"PRESIDENT\",None, \"17-11-1981\", 5000, None, 10),\n",
    "        (7844, \"Turner\", \"SALESMAN\",7698, \"08-09-1981\", 1500, 0,  30),\n",
    "        (7876, \"ADAMS\", \"CLERK\",   7788,  \"12-01-1983\", 1100, None, 20),\n",
    "        (7900, \"JAMES\",  \"CLERK\",  7698,  \"03-12-1981\", 950, None, 30),\n",
    "        (7902, \"FORD\",  \"ANALYST\", 7566,  \"03-12-1981\", 3000, None, 20),\n",
    "        (7934, \"MILLER\", \"CLERK\",  7782,  \"23-01-1982\", 1300, None, 10)\n",
    "       ]\n",
    "schema1 = [\"empno\",\"ename\",\"job\",\"mgr\",\"hiredate\",\"sal\",\"comm\",\"deptno\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a6ad4a-feea-484b-b66c-81286a3eaf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 10:01:26 WARN Utils: Your hostname, user-HP-Pavilion-x360-Convertible-14-dh0xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlo1)\n",
      "25/02/27 10:01:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/27 10:01:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-12-1980| 800|NULL|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|02-04-1981|2975|NULL|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-09-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-05-1981|2850|NULL|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|09-06-1981|2450|NULL|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|09-12-1982|3000|NULL|    20|\n",
      "| 7839|  KING|PRESIDENT|NULL|17-11-1981|5000|NULL|    10|\n",
      "| 7844|Turner| SALESMAN|7698|08-09-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|12-01-1983|1100|NULL|    20|\n",
      "| 7900| JAMES|    CLERK|7698|03-12-1981| 950|NULL|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|03-12-1981|3000|NULL|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-01-1982|1300|NULL|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n",
      "root\n",
      " |-- empno: long (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: long (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: long (nullable = true)\n",
      " |-- comm: long (nullable = true)\n",
      " |-- deptno: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "emp_df = spark.createDataFrame(data1,schema1)\n",
    "emp_df.show() \n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8137a58b-8f46-4250-8a56-f0005bd0cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38254800-c9dd-4644-b117-307f47945438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#emp_df = emp_df1.select(col(\"hiredate\"),to_date(col(\"hiredate\"),\"dd-MM-yyyy\").alias(\"to_date\"))\n",
    "#emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffabc8-87c9-41c1-b729-d1d60469bdba",
   "metadata": {},
   "source": [
    "# Determining the Number of Days Between Two Dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53b57d6-9244-4cab-8d84-d36a4ab86270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================>                     (7 + 4) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|datediff(ward_hd, allen_hd)|\n",
      "+---------------------------+\n",
      "|                       NULL|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql('''select datediff(ward_hd, allen_hd)\n",
    "              from\n",
    "             (select hiredate as ward_hd from emp\n",
    "                where ename='WARD') x,\n",
    "             (select hiredate as allen_hd from emp\n",
    "                where ename='ALLEN') y\n",
    "           ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eee3d31-d520-4c13-a104-c640d488f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981-02-20\n",
      "+----------+----------+\n",
      "|     start|       end|\n",
      "+----------+----------+\n",
      "|1981-02-20|1981-02-22|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+--------+\n",
      "|     start|       end|datediff|\n",
      "+----------+----------+--------+\n",
      "|1981-02-20|1981-02-22|       2|\n",
      "+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allen=emp_df.where(col(\"ename\")==\"ALLEN\").select(to_date(col(\"hiredate\"),\"dd-MM-yyyy\")).collect()[0][0]\n",
    "ward =emp_df.where(col(\"ename\")==\"WARD\").select(to_date(col(\"hiredate\"),\"dd-MM-yyyy\")).collect()[0][0] \n",
    "print(f\"{allen}\")\n",
    "data = [(allen,ward)]; schema = ['start','end'] \n",
    "df = spark.createDataFrame(data=data, schema=schema) \n",
    "df.show()\n",
    "df.withColumn(\"datediff\",datediff(col(\"end\"),col(\"start\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5191-f417-44d5-9667-59246ba2b161",
   "metadata": {},
   "source": [
    "# Determining the Number of Business Days Between Two Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358dbec-9e12-400e-973b-dbe8f19b1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(''' \n",
    "           select max(case when ename = 'BLAKE'\n",
    "                 then hiredate end) as blake_hd,\n",
    "                 max(case when ename = 'JONES'\n",
    "                 then hiredate end) as jones_hd\n",
    "           from emp \n",
    "           where ename in ('BLAKE','JONES')\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ff634-1107-4ceb-8297-1ab8d991a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select min(hiredate) AS min_hd,\n",
    "                  max(hiredate) AS max_hd\n",
    "                  from emp''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f97db-946c-46ce-8419-1a30ee8b0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hd = emp_df.agg({\"to_date\":\"min\"}).collect()[0][0]\n",
    "print(f\"min_hd {min_hd}\")\n",
    "max_hd = emp_df.agg({\"to_date\":\"max\"}).collect()[0][0]\n",
    "print(f\"max_hd {max_hd}\")\n",
    "data = [(min_hd,max_hd),]\n",
    "\n",
    "schema = [\"min_hd\",\"max_hd\"] \n",
    "df_minmax=spark.createDataFrame(data=data, schema=schema) \n",
    "df_minmax.show()\n",
    "df_minmax.withColumn(\"diff_days\", datediff(col(\"max_hd\") , col(\"min_hd\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90110330-5b38-4fdc-bebd-b02853662e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(min_hd, max_hd),] \n",
    "schema = [\"start2\",\"end2\"] \n",
    "df_date = spark.createDataFrame(data=data, schema=schema) \n",
    "df_date.show()\n",
    "df_date = df_date.withColumn(\"start_year\",year(col(\"start2\"))).withColumn(\"end_year\",year(col(\"end2\")))\n",
    "diff = df_date.withColumn(\"diff\",col(\"end_year\") - col(\"start_year\")).select('diff').collect()[0][0]\n",
    "print(f'diff={diff}')\n",
    "df_date = df_date.withColumn(\"start_month\",month(col(\"start2\"))).withColumn(\"end_month\",month(col(\"end2\")))\n",
    "diff_month = df_date.withColumn(\"diff_month\",col(\"end_month\") - col(\"start_month\")).select(\"diff_month\").collect()[0][0]\n",
    "#print(f'month {diff_month}')\n",
    "number_of_months = 3*12 + (-11) \n",
    "number_of_years  = number_of_months // 12\n",
    "print(f'Months:{number_of_months} Years : {number_of_years}')\n",
    "#df.withColumn(\"end_year\",year(\"end2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3886a5-9b99-4f4b-a3ed-d8f0bde7ffdb",
   "metadata": {},
   "source": [
    "# Determining the number of Months or Years Between Two Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7057c27-2484-4d62-b774-f8eb66766ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b048b23-bdeb-4c11-9447-e9cebbb4b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "val min_value = df.agg(min(\"salary\")).head().get(0)\n",
    "val max_value = df.agg(max(\"salary\")).head().get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79825af-d242-4744-b592-12f302934ec9",
   "metadata": {},
   "source": [
    "# Determining the Date Difference Between the Current Record and the Next Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4c290f-eb62-4d7e-a26c-e96c8d47ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|   to_date|\n",
      "+-----+------+---------+----+----------+----+----+------+----------+\n",
      "| 7782| CLARK|  MANAGER|7839|09-06-1981|2450|NULL|    10|1981-06-09|\n",
      "| 7839|  KING|PRESIDENT|NULL|17-11-1981|5000|NULL|    10|1981-11-17|\n",
      "| 7934|MILLER|    CLERK|7782|23-01-1982|1300|NULL|    10|1982-01-23|\n",
      "+-----+------+---------+----+----------+----+----+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 11:21:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/27 11:21:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/27 11:21:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+----------+-------------+------+\n",
      "|empno|ename|      job| mgr|  hiredate| sal|comm|deptno|   to_date|next_hiredate|noDays|\n",
      "+-----+-----+---------+----+----------+----+----+------+----------+-------------+------+\n",
      "| 7782|CLARK|  MANAGER|7839|09-06-1981|2450|NULL|    10|1981-06-09|   1981-11-17|   161|\n",
      "| 7839| KING|PRESIDENT|NULL|17-11-1981|5000|NULL|    10|1981-11-17|   1982-01-23|    67|\n",
      "+-----+-----+---------+----+----------+----+----+------+----------+-------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 11:21:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/27 11:21:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "windowspec = Window.orderBy(\"hiredate\")\n",
    "df=emp_df.where(emp_df.deptno==10)\n",
    "df=df.withColumn(\"to_date\",to_date(col(\"hiredate\"),\"dd-MM-yyyy\"))\n",
    "df.show() \n",
    "date_diff=df.withColumn(\"next_hiredate\",lead(\"to_date\",1).over(windowspec)).where(\"next_hiredate is not NULL\")\n",
    "#date_diff.show()\n",
    "date_diff.withColumn(\"noDays\",datediff(col(\"next_hiredate\"),col(\"to_date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12dfe9d-9547-4cbf-96a1-b85165bb25e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
